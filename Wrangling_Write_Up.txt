Data Wrangling Write Up

    Cleaning my data was a large task, as there were over 8000 rows and 1600 columns in my original data set.  Each row was an individual school and each column was an attribute of those schools.  While some columns were more basic qualities of the schools (i.e. size, location), others were extremely narrow and focused (i.eCertificate of at least one but less than two academic years in Natural Resources And Conservation).   The first step was cutting down the number of columns to something more reasonable, and particularly getting rid of overly specific columns that would likely not be useful to my project.  Forntunately the columns all fell within categories that helped seperate them from one another. These categories were academics, admissions, aid, completion, cost, earnings, repayment, root, school, and student.  By selecting some of the more encompassing columns from each category, I was able to find more general and useful attributes of the schools.
    There were dozens of columns concerning the percentage of students graduating in each general major (i.e. engineering, psychology, social sciences).  It was not reasonable to include each of these categories, but it was important to reflect the majors being given out by each of these universities.  I decided to incldue the 5 most popular major categories, determining these by averaging the percentage of students at each univeristy in the respective majors.  I added these five columns to my dataframe.  After selecting my attributes I would be using for modeling there were 14 total columns in my dataframe, and all rows remained.
    Ultimately the modeling done was to inform students looking at four year universities specifically.  As a result any schools that were not the four year format needed to be removed from the data set.  In order to do this I used a column in my smaller data set, Cost of Attending a Four Year University.  By dropping the rows in which this column was null, all schools that were not specifically four year instituions were removed from the dataset. Using this method also removed any four year school where the cost of attendence had not been recorded at all. After dropping those schools the dataset had roughly 4000 individual schools remaining in it. 
    At this point I also renamed each of my 14 columns to improve readability, as the orignial set did not have intuitive column names.  I also had to convert a few columns from object type to float.  The columns were stricly numeric aside from a few text entries that caused the type difference.  This text was denoting that the information was withheld for privacy reasons, and thus those values were converted to null for my data set.  Here I also removed all rows that did not contain my target variable, Average Earnings Six Years After Graduation, because those rows would not aid in the modeling process. At this point my data set had 3622 rows and 14 columns.
    Looking at the columns, two categories in general still had a large number of missing values, Average SAT score for accepted students and acceptance rate of the university. There was a third column with far fewer missing values, Completion percentage. After importing seaborn and matplotlib.pyplot, I made graphs to compare admission percentage and SAT score with my target variable of earnings.  SAT showed a stronger correlation with the target variable, while admission rate was only moderatly correlated. I also found that even though both columns had a large number of missing values, all rows (except a single one) that contained the SAT score also contained the admission percentage and completion percentage. On the other hand, there were many rows that contained admission rate and/or completion percentage but did not contain SAT score.  Because the SAT score was so strongly correlated with my target, I elected to create two data sets. OPne with only the 1200 rows containg SAT score, admission rate, and completion percentage, and one with all other rows. These were the final data sets I would be working with.
    
    